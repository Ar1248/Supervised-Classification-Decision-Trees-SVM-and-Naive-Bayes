{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Supervised Classification: Decision\n",
        "Trees, SVM, and Naive Bayes|"
      ],
      "metadata": {
        "id": "MEHPDdww_ePG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, and how is it used in Decision Trees ?\n",
        ">- Information Gain (IG) is a measure of how much uncertainty (entropy) is reduced in the target variable after splitting the data based on a feature.\n",
        "\n",
        "     It is used in Decision tree are:\n",
        "     >-For every feature, the tree calculates Information Gain for possible splits.\n",
        "\n",
        "   >-Higher Information Gain = Better split.\n",
        "\n",
        "   >-The decision tree chooses the feature (and threshold) with the maximum Information Gain to create the next node.\n",
        "\n",
        "2. What is the difference between Gini Impurity and Entropy ?\n",
        ">- Gini Impurity\n",
        "  a)Measures misclassification probability.\n",
        "  b)Gini is faster.\n",
        "  c)More sensitive to class probability changes\n",
        "  d)The feature that best separates dominant classes.\n",
        "\n",
        ">- Entropy\n",
        " a)Measures disorder or uncertainty in the data.\n",
        " b)Entropy is slower.\n",
        " c)More sensitive to rare classes due to log term.\n",
        " d)Splits that better capture minority classes.\n",
        "\n",
        "\n",
        "3. What is Pre-Pruning in Decision Trees ?\n",
        ">- Pre-pruning means stopping the tree from splitting further if the split does not improve model performance enough.\n",
        "\n",
        "4. Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances .\n",
        "\n"
      ],
      "metadata": {
        "id": "EJZexFts_2-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=0\n",
        ")\n",
        "\n",
        "# Train Decision Tree with Gini impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "# Print accuracy\n",
        "acc = model.score(X_test, y_test)\n",
        "print(\"\\nTest Accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv4yiAzME0AY",
        "outputId": "ded86a04-4df6-428e-dae4-f3907d58f2d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0215\n",
            "petal length (cm): 0.3977\n",
            "petal width (cm): 0.5808\n",
            "\n",
            "Test Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is a Support Vector Machine (SVM) ?\n",
        ">- A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression, mainly known for its strong performance in classification tasks.\n",
        "\n",
        "6. What is the Kernel Trick in SVM ?\n",
        ">- The Kernel Trick in SVM is a method that allows the algorithm to separate data that is not linearly separable by implicitly mapping it into a higher-dimensional space — without ever computing that space explicitly.\n",
        "\n",
        "7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n"
      ],
      "metadata": {
        "id": "4hHQrxCEFBfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "acc_linear = svm_linear.score(X_test, y_test)\n",
        "\n",
        "# SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "acc_rbf = svm_rbf.score(X_test, y_test)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Accuracy Comparison on Wine Dataset:\")\n",
        "print(f\"Linear Kernel Accuracy: {acc_linear:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy:    {acc_rbf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaLbHHOgFwQP",
        "outputId": "6ded1dc3-ac0d-4376-ee5b-d21e782f0716"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison on Wine Dataset:\n",
            "Linear Kernel Accuracy: 0.9815\n",
            "RBF Kernel Accuracy:    0.7593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Naïve Bayes classifier, and why is it called \"Naïve\" ?\n",
        ">- A Naïve Bayes classifier is a probabilistic machine learning model based on Bayes’ Theorem, used mainly for classification tasks.\n",
        "\n",
        "9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        ">- Gaussian Naïve Bayes - a)Continuous values b)Iris dataset, sensor data c) Example : Height, temperature\n",
        "\n",
        ">- Multinomial NB - a) Counts, frequencies b) Text classification (BoW, TF-IDF) c) Example : Word counts\n",
        "\n",
        ">- Bernoulli NB - a)Binary (0/1) features b) Text with presence/absence c) binary presence/absence features.\n",
        "\n",
        "10. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "miSkrO2PGEv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = gnb.score(X_test, y_test)\n",
        "\n",
        "print(\"Gaussian Naïve Bayes Accuracy on Breast Cancer Dataset:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5343OkAhNs1k",
        "outputId": "d91eeef4-cafe-449c-e7cc-7eec010ecc79"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes Accuracy on Breast Cancer Dataset: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}